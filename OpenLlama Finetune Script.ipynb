{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4151600a-e389-47b7-8a0b-88d17a2fe792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug 24 02:11:21 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:1B.0 Off |                    0 |\n",
      "| N/A   37C    P0    50W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  Off  | 00000000:00:1C.0 Off |                    0 |\n",
      "| N/A   35C    P0    49W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  Off  | 00000000:00:1D.0 Off |                    0 |\n",
      "| N/A   36C    P0    53W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  Off  | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   38C    P0    54W / 300W |      0MiB / 16160MiB |      2%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check instance GPUs\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8efb1ab1-152a-4d49-9484-d505f32e8fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "# Install required libraries\n",
    "%pip install -q huggingface_hub\n",
    "%pip install -q -U trl transformers accelerate bitsandbytes xformers\n",
    "%pip install -q -U datasets einops wandb evaluate sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d411f3f-e4f9-4d3e-9fe8-71d29bea73a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install git lfs\n",
    "%conda install --yes -c conda-forge git-lfs\n",
    "\n",
    "%git lfs install --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfef6f5c-b277-43fc-bfca-13094ebc736c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.25.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import wandb\n",
    "from huggingface_hub import notebook_login\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, LlamaTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bc60a77-6139-4be8-ae75-a451bc435436",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mingeniousartist\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log in to WandB\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10dd2470-d8bc-48a0-8129-8831b2c6e849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=sentiment_finance\n"
     ]
    }
   ],
   "source": [
    "# Create WandB project\n",
    "%env WANDB_PROJECT=sentiment_finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f19fdb2a-4597-41d6-8558-601ac798caca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd4976b9273c46c09d93dc3604a7cb86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Log in to your Hugging Face account\n",
    "# Get your API token here https://huggingface.co/settings/token\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20cef19e-9b91-498f-8394-5e6d3af3db25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"financial_phrasebank\",\"sentences_50agree\", split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0235d3b1-212a-453f-9651-2812db3d51e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': 'According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .', 'label': 1}\n",
      "{'sentence': 'The ongoing project where Tekla Structures is being used is the Vashi Exhibition Centre being developed by Insteel Engineers Pvt Ltd-IIVRCL Infrastructures & Projects Ltd & CIDCO .', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "# Create train and test datasets\n",
    "train_dataset = dataset.select([i for i in list(range(0,3873))])\n",
    "test_dataset = dataset.select([i for i in list(range(3873,4841))])\n",
    "\n",
    "print(train_dataset[0])\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab1c6ba2-c734-4c8a-866a-314dcd5179ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Metric\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffd98850-c90c-46aa-92be-5e0a7891febf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create compute metrics class\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "    # Convert predictions and labels into int32 from int64\n",
    "    predictions = predictions.astype(np.int32)\n",
    "    labels = labels.astype(np.int32)\n",
    "    \n",
    "    # Compute metric\n",
    "    accuracy_res = accuracy.compute(predictions=predictions, references=labels)\n",
    "    return accuracy_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab3d00f7-dcce-4f38-a2e9-0dd97966b34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define StableLM-base as our base model:\n",
    "base_model_name = \"openlm-research/open_llama_3b_v2\"\n",
    "\n",
    "# Add names of multiclass labels\n",
    "label2id = {\n",
    "    \"Negative\": 0,\n",
    "    \"Neutral\": 1,\n",
    "    \"Positive\": 2\n",
    "  }\n",
    "id2label = {\n",
    "    \"0\": \"Negative\",\n",
    "    \"1\": \"Neutral\",\n",
    "    \"2\": \"Positive\"\n",
    "  }\n",
    "\n",
    "# Add labels to model config\n",
    "config = AutoConfig.from_pretrained(base_model_name, label2id=label2id, id2label=id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "394781d3-0cae-471e-b0fd-a85160e3938d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set device map according to instance\n",
    "device_map = \"auto\"\n",
    "# device_map = {'': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64f909c4-6df3-4dd2-816d-76c505a38946",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:460: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at openlm-research/open_llama_3b_v2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Create model object\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True,\n",
    "    use_auth_token=True,\n",
    "    config=config,\n",
    "    # max_memory=f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB' # For avoiding CUDA out of memory (single GPU)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3249585-0ea5-4918-9de4-ee77478d3d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more settings to model config\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "282df08f-bd23-4b54-8044-cd27a302604d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3324083200 || all params: 3324083200 || trainable%: 100.0\n"
     ]
    }
   ],
   "source": [
    "# Create a function to get parameters of the model\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb9a7b64-94a0-41aa-b56b-ff26353e2231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 8479, 420, 325, 260, 1248]\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer object from pre-trained model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"openlm-research/open_llama_3b_v2\")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"openlm-research/open_llama_3b_v2\")\n",
    "\n",
    "# convert all the model weights to half precision\n",
    "#model.half().cuda()\n",
    "\n",
    "# Add padding and test tokenizer\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(tokenizer.encode(\"Hello this is a test\"))\n",
    "\n",
    "# Add padding to model config\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95b9af8f-c7b8-4f51-93b8-ba4f9ba66d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenized train and test datasets\n",
    "\n",
    "train_dataset = train_dataset.map(lambda samples: tokenizer(samples[\"sentence\"]), batched=True)\n",
    "test_dataset = test_dataset.map(lambda samples: tokenizer(samples[\"sentence\"]), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5135d58c-4398-4558-b054-dea07102d53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data collator with padding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bae6de6-0e22-49f5-9d13-eea44c362f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set repository name and output folder\n",
    "\n",
    "repo_name = \"openllama-3b-finance\"\n",
    "output_dir = repo_name\n",
    "\n",
    "# Set training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    \n",
    "    # Low batch size to avoid OOM\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=20, # Logs every 20 steps and measures everything\n",
    "    weight_decay=0.001,\n",
    "    eval_accumulation_steps=10, # Low accumulation to avoid OOM\n",
    "    num_train_epochs=1,\n",
    "    lr_scheduler_type = \"cosine\", # Schedules linear/cosine\n",
    "    evaluation_strategy=\"steps\", # Evaluates every 20 steps\n",
    "    save_strategy=\"no\",\n",
    "    #save_steps = 500,\n",
    "    seed = 42,\n",
    "    save_safetensors = True,\n",
    "    push_to_hub=True,\n",
    "    gradient_checkpointing = True,\n",
    "    auto_find_batch_size = True, # Good for avoiding OOM\n",
    "    optim=\"paged_adamw_8bit\", # Optimizer\n",
    "    \n",
    "    # other args and kwargs here\n",
    "    report_to=\"wandb\",  # enable logging to W&B\n",
    "    run_name=\"openllama-3b (cosine)\",  # name of the W&B run (optional)\n",
    ")\n",
    "\n",
    "# Set Sequence length\n",
    "max_seq_length = 512\n",
    "\n",
    "# Initialize trainer object with args and dataset\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d219a9d-606e-4cc5-8e05-d804579745d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/wandb/run-20230824_022001-aczugabz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ingeniousartist/sentiment_finance/runs/aczugabz' target=\"_blank\">openllama-3b (cosine)</a></strong> to <a href='https://wandb.ai/ingeniousartist/sentiment_finance' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ingeniousartist/sentiment_finance' target=\"_blank\">https://wandb.ai/ingeniousartist/sentiment_finance</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ingeniousartist/sentiment_finance/runs/aczugabz' target=\"_blank\">https://wandb.ai/ingeniousartist/sentiment_finance/runs/aczugabz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3873' max='3873' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3873/3873 3:03:53, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>21.965500</td>\n",
       "      <td>8.166256</td>\n",
       "      <td>0.081612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.231000</td>\n",
       "      <td>6.300709</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.745200</td>\n",
       "      <td>4.089234</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.456100</td>\n",
       "      <td>5.031425</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.337000</td>\n",
       "      <td>5.617582</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>3.222600</td>\n",
       "      <td>4.496350</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.563300</td>\n",
       "      <td>6.179950</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.476400</td>\n",
       "      <td>4.705860</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.450000</td>\n",
       "      <td>5.060175</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.423200</td>\n",
       "      <td>5.341760</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.768400</td>\n",
       "      <td>5.180456</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.706500</td>\n",
       "      <td>4.756809</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.341700</td>\n",
       "      <td>6.106238</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.907000</td>\n",
       "      <td>12.098791</td>\n",
       "      <td>0.504132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>14.604300</td>\n",
       "      <td>3.028285</td>\n",
       "      <td>0.081612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.337000</td>\n",
       "      <td>12.778611</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>4.182000</td>\n",
       "      <td>7.561935</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>3.736500</td>\n",
       "      <td>7.858081</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>3.209000</td>\n",
       "      <td>3.254740</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.483600</td>\n",
       "      <td>89.852539</td>\n",
       "      <td>0.081612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>4.580500</td>\n",
       "      <td>103.076225</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>4.635100</td>\n",
       "      <td>91.450104</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>11.087300</td>\n",
       "      <td>88.046906</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.127400</td>\n",
       "      <td>86.712990</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.039800</td>\n",
       "      <td>86.418564</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>18.692400</td>\n",
       "      <td>80.149139</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>1.221600</td>\n",
       "      <td>76.842918</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>1.117900</td>\n",
       "      <td>78.015854</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>10.098100</td>\n",
       "      <td>71.111359</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>9.012300</td>\n",
       "      <td>66.294456</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>1.953900</td>\n",
       "      <td>65.685371</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>8.472900</td>\n",
       "      <td>62.159454</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>7.816000</td>\n",
       "      <td>52.076286</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>6.044300</td>\n",
       "      <td>41.150043</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.180400</td>\n",
       "      <td>42.800652</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>1.612200</td>\n",
       "      <td>44.097591</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>9.892700</td>\n",
       "      <td>31.638086</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>6.828000</td>\n",
       "      <td>12.748254</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>3.145700</td>\n",
       "      <td>13.298075</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.999100</td>\n",
       "      <td>12.484590</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>2.553900</td>\n",
       "      <td>13.766862</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>1.389800</td>\n",
       "      <td>12.891864</td>\n",
       "      <td>0.081612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>2.925100</td>\n",
       "      <td>15.914906</td>\n",
       "      <td>0.081612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>4.087400</td>\n",
       "      <td>10.528213</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.476300</td>\n",
       "      <td>3.028098</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>2.286500</td>\n",
       "      <td>12.246029</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>4.243800</td>\n",
       "      <td>10.196064</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>2.547000</td>\n",
       "      <td>1.409899</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.865900</td>\n",
       "      <td>8.321655</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.533100</td>\n",
       "      <td>6.398963</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>2.470400</td>\n",
       "      <td>2.233747</td>\n",
       "      <td>0.081612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>2.138100</td>\n",
       "      <td>10.626286</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>1.592700</td>\n",
       "      <td>11.198907</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>2.485000</td>\n",
       "      <td>8.817443</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.807400</td>\n",
       "      <td>5.597088</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.862200</td>\n",
       "      <td>5.508899</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>2.808500</td>\n",
       "      <td>5.430011</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>1.240500</td>\n",
       "      <td>7.565669</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>3.937400</td>\n",
       "      <td>2.717954</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.749400</td>\n",
       "      <td>4.963909</td>\n",
       "      <td>0.081612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>2.609400</td>\n",
       "      <td>2.198022</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>2.207200</td>\n",
       "      <td>7.339211</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.997800</td>\n",
       "      <td>7.912731</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>2.387200</td>\n",
       "      <td>7.061347</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>3.312900</td>\n",
       "      <td>4.420234</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>1.776000</td>\n",
       "      <td>6.146651</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>3.117900</td>\n",
       "      <td>6.060713</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>1.272000</td>\n",
       "      <td>5.048374</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>3.069400</td>\n",
       "      <td>3.166489</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.945200</td>\n",
       "      <td>4.869191</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>2.368900</td>\n",
       "      <td>4.937484</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>2.708200</td>\n",
       "      <td>3.210830</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>0.824400</td>\n",
       "      <td>7.015069</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>2.603200</td>\n",
       "      <td>5.564500</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.874500</td>\n",
       "      <td>4.240774</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>6.879981</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>2.533500</td>\n",
       "      <td>6.310932</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>2.549500</td>\n",
       "      <td>4.401716</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>1.723400</td>\n",
       "      <td>5.173936</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.106600</td>\n",
       "      <td>6.076890</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>2.554100</td>\n",
       "      <td>3.753876</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>2.459800</td>\n",
       "      <td>4.207541</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>1.721100</td>\n",
       "      <td>5.397515</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>2.399300</td>\n",
       "      <td>4.142728</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.616100</td>\n",
       "      <td>5.087119</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>2.236100</td>\n",
       "      <td>4.337524</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>2.084100</td>\n",
       "      <td>4.735698</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>2.137000</td>\n",
       "      <td>5.273738</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>2.381900</td>\n",
       "      <td>3.168843</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.639100</td>\n",
       "      <td>5.616876</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>1.276000</td>\n",
       "      <td>6.194546</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>2.069400</td>\n",
       "      <td>6.376128</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>2.371500</td>\n",
       "      <td>6.166590</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>2.142800</td>\n",
       "      <td>6.471848</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.040900</td>\n",
       "      <td>6.325928</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>2.192400</td>\n",
       "      <td>6.085326</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>2.351100</td>\n",
       "      <td>4.719894</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>2.733500</td>\n",
       "      <td>4.359138</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>1.678400</td>\n",
       "      <td>3.748809</td>\n",
       "      <td>0.161157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.552500</td>\n",
       "      <td>6.049654</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>2.745700</td>\n",
       "      <td>3.595238</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>2.392900</td>\n",
       "      <td>4.768421</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>1.952200</td>\n",
       "      <td>5.639449</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>2.225700</td>\n",
       "      <td>4.580054</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.675300</td>\n",
       "      <td>5.052088</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2120</td>\n",
       "      <td>1.615400</td>\n",
       "      <td>5.472961</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2140</td>\n",
       "      <td>1.772300</td>\n",
       "      <td>5.525136</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>2.696300</td>\n",
       "      <td>3.509771</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2180</td>\n",
       "      <td>1.727400</td>\n",
       "      <td>5.426207</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.405900</td>\n",
       "      <td>4.501935</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>1.650500</td>\n",
       "      <td>5.110663</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>1.246900</td>\n",
       "      <td>5.345599</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2260</td>\n",
       "      <td>1.670200</td>\n",
       "      <td>5.410295</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>1.615000</td>\n",
       "      <td>5.802418</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.562200</td>\n",
       "      <td>5.603476</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>2.353600</td>\n",
       "      <td>5.377885</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2340</td>\n",
       "      <td>2.051200</td>\n",
       "      <td>5.249806</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2360</td>\n",
       "      <td>2.140500</td>\n",
       "      <td>5.227928</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2380</td>\n",
       "      <td>2.192600</td>\n",
       "      <td>4.326044</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.399500</td>\n",
       "      <td>4.444498</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2420</td>\n",
       "      <td>1.494400</td>\n",
       "      <td>4.961603</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2440</td>\n",
       "      <td>2.662300</td>\n",
       "      <td>4.973563</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2460</td>\n",
       "      <td>1.409500</td>\n",
       "      <td>4.650609</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2480</td>\n",
       "      <td>2.480300</td>\n",
       "      <td>4.097075</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.272100</td>\n",
       "      <td>4.319205</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>1.837200</td>\n",
       "      <td>4.490750</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2540</td>\n",
       "      <td>1.894200</td>\n",
       "      <td>4.732273</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2560</td>\n",
       "      <td>2.140700</td>\n",
       "      <td>4.955376</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2580</td>\n",
       "      <td>2.503900</td>\n",
       "      <td>5.159933</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.732100</td>\n",
       "      <td>5.608873</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2620</td>\n",
       "      <td>2.062100</td>\n",
       "      <td>4.835948</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>2.166400</td>\n",
       "      <td>4.558095</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2660</td>\n",
       "      <td>1.883500</td>\n",
       "      <td>5.102877</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2680</td>\n",
       "      <td>3.031400</td>\n",
       "      <td>3.958704</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.178100</td>\n",
       "      <td>4.458446</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2720</td>\n",
       "      <td>3.322200</td>\n",
       "      <td>4.762820</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2740</td>\n",
       "      <td>2.118400</td>\n",
       "      <td>4.403866</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>1.929300</td>\n",
       "      <td>3.875461</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2780</td>\n",
       "      <td>2.244800</td>\n",
       "      <td>4.432662</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.469700</td>\n",
       "      <td>3.302560</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2820</td>\n",
       "      <td>1.856900</td>\n",
       "      <td>3.772182</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2840</td>\n",
       "      <td>0.854400</td>\n",
       "      <td>4.917575</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2860</td>\n",
       "      <td>2.244500</td>\n",
       "      <td>4.388884</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>1.372300</td>\n",
       "      <td>4.328029</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>2.216700</td>\n",
       "      <td>4.401565</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2920</td>\n",
       "      <td>1.980000</td>\n",
       "      <td>3.866090</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2940</td>\n",
       "      <td>1.734400</td>\n",
       "      <td>3.791891</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2960</td>\n",
       "      <td>1.924000</td>\n",
       "      <td>4.140794</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2980</td>\n",
       "      <td>1.381100</td>\n",
       "      <td>4.372986</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.828900</td>\n",
       "      <td>4.287199</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3020</td>\n",
       "      <td>1.957300</td>\n",
       "      <td>4.616508</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3040</td>\n",
       "      <td>2.487700</td>\n",
       "      <td>4.598808</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3060</td>\n",
       "      <td>1.174900</td>\n",
       "      <td>4.788683</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3080</td>\n",
       "      <td>2.183500</td>\n",
       "      <td>4.901813</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>2.375200</td>\n",
       "      <td>4.691126</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>1.974100</td>\n",
       "      <td>4.512636</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3140</td>\n",
       "      <td>1.751300</td>\n",
       "      <td>4.625075</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3160</td>\n",
       "      <td>3.066600</td>\n",
       "      <td>4.025976</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3180</td>\n",
       "      <td>0.556900</td>\n",
       "      <td>4.096462</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.180500</td>\n",
       "      <td>4.523973</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3220</td>\n",
       "      <td>2.431900</td>\n",
       "      <td>4.307983</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3240</td>\n",
       "      <td>2.126000</td>\n",
       "      <td>3.782267</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3260</td>\n",
       "      <td>1.699300</td>\n",
       "      <td>3.809319</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3280</td>\n",
       "      <td>0.686100</td>\n",
       "      <td>4.161833</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.748000</td>\n",
       "      <td>4.565273</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3320</td>\n",
       "      <td>2.572100</td>\n",
       "      <td>4.662844</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3340</td>\n",
       "      <td>2.013700</td>\n",
       "      <td>4.279631</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3360</td>\n",
       "      <td>2.186400</td>\n",
       "      <td>4.117340</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3380</td>\n",
       "      <td>2.488100</td>\n",
       "      <td>3.961742</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.683700</td>\n",
       "      <td>3.757534</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3420</td>\n",
       "      <td>1.595100</td>\n",
       "      <td>3.608591</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3440</td>\n",
       "      <td>2.504000</td>\n",
       "      <td>3.591887</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3460</td>\n",
       "      <td>1.498200</td>\n",
       "      <td>3.751925</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3480</td>\n",
       "      <td>1.899400</td>\n",
       "      <td>3.712009</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.612600</td>\n",
       "      <td>3.685402</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3520</td>\n",
       "      <td>2.002000</td>\n",
       "      <td>3.788809</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3540</td>\n",
       "      <td>1.026400</td>\n",
       "      <td>3.799016</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3560</td>\n",
       "      <td>1.949500</td>\n",
       "      <td>3.963501</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3580</td>\n",
       "      <td>2.074200</td>\n",
       "      <td>3.965078</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.780300</td>\n",
       "      <td>3.951757</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3620</td>\n",
       "      <td>2.084300</td>\n",
       "      <td>3.940441</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3640</td>\n",
       "      <td>1.843100</td>\n",
       "      <td>3.933426</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3660</td>\n",
       "      <td>1.498700</td>\n",
       "      <td>3.960905</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3680</td>\n",
       "      <td>1.821400</td>\n",
       "      <td>4.006040</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.096400</td>\n",
       "      <td>4.042250</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3720</td>\n",
       "      <td>0.966900</td>\n",
       "      <td>4.054877</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3740</td>\n",
       "      <td>1.622600</td>\n",
       "      <td>4.048560</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3760</td>\n",
       "      <td>1.806100</td>\n",
       "      <td>4.040454</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3780</td>\n",
       "      <td>2.873800</td>\n",
       "      <td>4.031717</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.684000</td>\n",
       "      <td>4.031950</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3820</td>\n",
       "      <td>1.115800</td>\n",
       "      <td>4.030266</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3840</td>\n",
       "      <td>1.775000</td>\n",
       "      <td>4.029400</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3860</td>\n",
       "      <td>2.163900</td>\n",
       "      <td>4.029638</td>\n",
       "      <td>0.414256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3873, training_loss=2.663619207160097, metrics={'train_runtime': 11048.6646, 'train_samples_per_second': 0.351, 'train_steps_per_second': 0.351, 'total_flos': 2509046876160000.0, 'train_loss': 2.663619207160097, 'epoch': 1.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model and evaluate according to strategy\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "822e883e-7239-4abd-a5db-4e5778d9f869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empties GPU cache to avoid OOM\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63f36754-b278-4171-a7fc-360d928db6b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da27739617dc4adc93746363114dedd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.014 MB of 0.037 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.377923…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>████▁███████▁███████████████████████████</td></tr><tr><td>eval/loss</td><td>▁▁▁▂█▇▆▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▆▂▂▂▂▂▇▇▇▇▃▂▂▂▃▂▂▃▃▂▃▂▂▂▁▄▃▂▄▂▇███▃▃▃▃▂▃</td></tr><tr><td>eval/samples_per_second</td><td>▃▇▇▇▇▇▂▂▂▂▆▆▇▇▆▇▇▆▆▆▆▆▇▇█▄▆▇▅▇▂▁▁▁▆▆▆▆▇▆</td></tr><tr><td>eval/steps_per_second</td><td>▃▇▇▇▇▇▂▂▂▂▆▆▇▇▆▇▇▆▆▆▆▆▇▇█▄▆▇▅▇▂▁▁▁▆▆▆▆▇▆</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>▂▂▁▁▂█▄▁▁▂▂▂▁▂▁▂▂▂▂▂▂▂▂▁▂▂▂▂▁▁▁▂▁▁▂▂▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.41426</td></tr><tr><td>eval/loss</td><td>4.02964</td></tr><tr><td>eval/runtime</td><td>51.711</td></tr><tr><td>eval/samples_per_second</td><td>18.719</td></tr><tr><td>eval/steps_per_second</td><td>18.719</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>3873</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.1639</td></tr><tr><td>train/total_flos</td><td>2509046876160000.0</td></tr><tr><td>train/train_loss</td><td>2.66362</td></tr><tr><td>train/train_runtime</td><td>11048.6646</td></tr><tr><td>train/train_samples_per_second</td><td>0.351</td></tr><tr><td>train/train_steps_per_second</td><td>0.351</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">openllama-3b (cosine)</strong> at: <a href='https://wandb.ai/ingeniousartist/sentiment_finance/runs/aczugabz' target=\"_blank\">https://wandb.ai/ingeniousartist/sentiment_finance/runs/aczugabz</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230824_022001-aczugabz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finishes run logging on WandB\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98e7555e-b2ad-4fe6-82e8-d3b56231339a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('openllama-3b-finance/final_checkpoint/tokenizer_config.json',\n",
       " 'openllama-3b-finance/final_checkpoint/special_tokens_map.json',\n",
       " 'openllama-3b-finance/final_checkpoint/tokenizer.model',\n",
       " 'openllama-3b-finance/final_checkpoint/added_tokens.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saves final checkpoint, model and tokenizer\n",
    "\n",
    "output_dir = os.path.join(output_dir, \"final_checkpoint\")\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b7ac1c9-e012-499e-84fa-626c6876583b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468f6f73216a455db05704a5acb03ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc49fa8727945ce91bd87e0d74962e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/4.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47306a8faee24203ab58fdab240135cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5789edfd26484f8d8284d7ef23496ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.31G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/IngeniousArtist/openllama-3b-finance/tree/main/'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload the model to the Hub\n",
    "# model.push_to_hub(\"openllama-3b-finance\")\n",
    "# tokenizer.push_to_hub(\"openllama-3b-finance\")\n",
    "trainer.push_to_hub(\"openllama-3b-finance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd426b7-9132-47b4-a9f6-cdf0423a8625",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-2:712779665605:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
